Nowadays, data stream from every day life. From social applications to cities infrastructures, from wearable devices to sensor-equipped buildings and vehicles, there have been very strong advances related to the data generation. Furthermore, the advances related to data collection came together with the possibility of storing data which we would have trashed in the past. The reasons behind this new trend (i.e. collecting as much data as possible) concern the new value that is given to such data. Indeed, the "big data revolution" is not meant to describe just the quantity of data. The revolution strongly affects the new value that is given to such data.  
%We have witnessed, at the same moment, very strong advances in the domain of data generation, data collection and data storage.

%
%Just think about the new social applications which gathers information about every possible aspects of the users. From the voluntary data (tweets, comments, pictures) to data extracted with less straightforward techniques (cookies, pointer tracking, machine learning algorithms applied to photo repositories,...). What about the data generated by the wearable devices, or by the car black-boxes installed by car insurances on the customers' cars?
%The advances related to data generation and collection came together with the possibility of storing data which we would have trashed in the past. The reason behind this new trend about gathering as much data as one can is related to the new value that is given to such data.
%Everybody are collecting data because it is useful. And if it is not clear how can be exploited now, probably it will be useful in the future.
%Lying hidden in all this raw data is potentially useful knowledge, which is rarely exploited. 

The value of these data is directly correlated to the knowledge which can be extracted from it, related to the specific use case. The common story follows a pattern in which human qualitative domain experts are outperformed by the results of a modern data analysis algorithm which can leverage a sufficient amount of data (often, this algorithm was developed by a researcher who does not know anything about that specific domain).

Thanks to big data analysis, currently, many companies are able to develop predictive models which target customers. This is possible thanks to the analysis of huge amount of customer attributes. By means of municipal data collections, in urban scenarios, crimes are predicted or interesting correlations between health and air quality are extracted. The information collected by sensors in the automotive environment, instead, is leveraged in many research domains: from self-driving algorithms training to predictive component replacement. Finally, big data analysis has already been very useful in human genome's researches. 


The branch of computer science whose analytic tools are used to transform these huge collections of data into effective and useful knowledge is called \textit{data mining}.
In the last years, the interest towards data mining has risen. The trend is noticeable in both industrial and academic environments. For companies, as already discussed, it represents a very powerful source of information. In~\cite{junque2013predictive}, the authors present a study to illustrate that
larger data indeed can be more valuable assets for predictive analytics. The
deduction is that institutions with larger collections of data and, of course,
the skill to take advantage of them, can obtain a competitive advantage over
institutions without. 
On the other hand, from the academic point of view, the design of big data algorithms represent a very stimulant challenge and research opportunity. Indeed, the application of traditional data mining techniques to such large collection of data is very challenging. As the amount of data increases, the proportion of it that people is able to interpret decreases \cite{witten2005data}. For this reason, there is a concrete need of a new generation of scalable tools which, often, require to be redesigned from scratches to cope with such an extreme and complex environment.

In this dissertation, we focus on one of the most popular data mining techniques, frequent itemset mining. Frequent itemset mining is an exploratory data analysis method used to discover frequent co-occurrence among the items of a transactional dataset (attribute-value pairs). Frequent itemsets are very useful for data summarization and correlation analysis. 
Frequent co-occurent patterns are commonly used to identify the most relevant insights from large collection of data which cannot be manually examined because of their size. 
Itemsets are also used to generate association rules, which highlights and analyze relations between objects.

Frequent itemset extraction is a very challenging problem in the big data domain. The reason is related to the nature of the problem which requires a full knowledge of the input data. In the last years several scalable techniques have been introduced. All of them relies on different search space exploration strategy and this leads to different performances related to the use case. 
\\

\textbf{Thesis statement: }\textit{The target of this dissertation is to thoroughly analyze the scalable frequent itemset mining environment and, eventually, making a step forward to fill in the discovered gap.}

In the final part of this Chapter, we resume this dissertation plan highlighting our research contribution.

\section{Dissertation plan and research contribution}
The first contribution of this dissertation is the deep analysis the current state of the art of frequent itemset mining algorithms. After the exploration of the domain and the discovery of possible lacks or issues, the thesis will describe our attempt to enrich the former with new solutions and algorithms. 

The aforementioned dissertation target is achieved through three main steps, useful to cluster together and label the research contributions behind them:

\begin{enumerate}
\item A deep analysis of the most reliable frequent itemset mining tools for big data, trying to discover possible lacks or issues
%\item The introduction of a new scalable frequent itemset mining algorithm
\item The enrichment of the scalable frequent pattern mining environment with a new distributed high-dimensional algorithm 
\item The development of a big data mining framework in which, through distributed frequent itemset mining, a new type of itemsets are extracted
\end{enumerate}
The remainder part of this section will briefly introduce each phase in order to deliver a clear idea of the structure of the dissertation work.




\subsection{Frequent Itemset Mining for Big Data: a comparative analysis}
%Itemset mining is a well-known exploratory data mining technique used to
%discover interesting correlations hidden in a data collection. Since it supports
%different targeted analyses, it is profitably exploited in a wide range
%of different domains, ranging from retail store informations to network traffic or biological repositories. 
As already mentioned, with the increasing amount of generated data, different distributed and scalable frequent itemset algorithms
have been developed. They have been designed exploiting the computational advantages of distributed
computing platforms, such as Apache Hadoop and Apache Spark.

This Section reviews scalable algorithms addressing the frequent itemset mining 
problem in the Big Data frameworks through both theoretical and experimental 
comparative analyses \cite{apiletti2015review},\cite{survey_pulvi}. 
Since the itemset mining task is computationally expensive, its distribution and 
parallelization strategies heavily affect memory usage, load balancing, 
and communication costs.  
A detailed discussion of the algorithmic choices of the  
distributed methods for frequent itemset mining is followed by an experimental 
analysis comparing the performance of state-of-the-art distributed implementations on both 
synthetic and real datasets. 
The strengths and weaknesses of the algorithms are
thoroughly discussed with respect 
to the dataset features (e.g., data distribution, average 
transaction length, number of records), 
and specific parameter settings. 
Finally, based on theoretical and experimental analyses, open 
research directions for the parallelization of the itemset mining problem 
are presented.
The takeaways of the review is that no algorithm is universally superior and performances are heavily skewed by the use cases and the relative input data. Additionally, the experiments have highlighted the fundamental importance of Load Balancing, even sacrificing Communication Costs, which, in this scenario, can be considered as a price worth paying.
Finally, all of the algorithms focus on being able to deal with a huge number of transactions. None of them has been designed to cope with a huge number of attributes, i.e. high-dimensional data. As shown in the next subsection, we have tried to fill in this gap.
%
%
% However, depending on the use case, it is not easy to select the best fitting algorithm. Several features affects this choice, such as data cardinality or data distribution. Therefore, the algorithm selection often relies on analyst expertise. 
%For this reason, the delivered analysis will examine both theoretically \cite{apiletti2015review} and experimentally \cite{survey_pulvi} some state-of-the-art implementations of frequent itemset mining algorithms. The ratio is to guide the analyst in selecting the most suitable approach based on the use case and the outline lesson learned. 
%The review takes into account also some aspects typical of distributed environment, such as communication costs and load balancing. Many real and synthetic datasets have been considered in the comparison.
%
%The takeaways of the review is that no algorithm is universally superior and performances are heavily skewed by the use cases and the relative input data. However, it is very clear that all of the algorithms focus on being able to deal with a huge number of transactions. None of them has been designed to cope with a huge number of attributes, i.e. high-dimensional data. As shown in the next subsection, we have tried to fill in this gap.

\subsection{A Parallel Map-Reduce Algorithm to Efficiently
Support Itemset Mining on High Dimensional Data}
%In today's world, many scientific applications such as bioinformatics and networking, are continuously generated large volumes of data. Since
%each monitored event is usually characterized by a variety of features, high-dimensional datasets have been continuously generated. 
%Frequent itemset is one of the technique used to extract value from these complex collections, discovering hidden and non-trivial correlations among data. 
Thanks to the spread of distributed and parallel frameworks, the development of scalable approaches able to deal with the so called Big Data has been extended
to frequent itemset mining. Unfortunately, as mentioned in the previous Subsection (and clearly shown in Chapter \ref{survey}), most of the current algorithms are designed to cope with low-dimensional datasets, delivering poor performances in those use cases characterized by high-dimensional data. Chapter \ref{pampa} introduces 
PaMPa-HD \cite{pampa_v1},\cite{pampa_pulvi}, a MapReduce-based frequent closed itemset mining
algorithm for high dimensional datasets. An efficient solution has been pro-
posed to parallelize and speed up the mining process. Furthermore, different
strategies have been proposed to easily tune-up the algorithm parameters.
The experimental results, performed on real-life high-dimensional use cases,
show the efficiency of the proposed approach in terms of execution time, load
balancing and robustness to memory issues.

\subsection{Big Data Mining frameworks and Misleading Generalized Itemsets}
The analysis of data often includes a large number of steps; frequent itemset extraction is often just one of the required processes to extract the desired knowledge from raw data. 

%Data analysis is composed b very large family of processes; frequent itemset mining represents just one of the steps required to deal with data. Along with other data mining algorithms, they represent just the knowledge extraction and exploration step of the whole process, which is strongly composed of many data preparation phases.
The availability of distributed and parallel platforms has allowed not only the spread of scalable algorithms and tools, but also the design of complete big data mining systems like the one presented in \cite{baralis2014nemico}. These systems design fits large amount of data starting from the very first data preparation steps.

In these frameworks, distributed frequent itemset mining is just one of the possible steps or 'modules'. It can be replaced by other data analyses or used to support further data mining processes. In Chapter \ref{nemico} we will introduce a big data mining framework effective for a wide range of analyses. We will specifically focus on the extraction of 'misleading generalized itemset' \cite{6924449}, a particular type of itemsets obtained from frequent itemsets and a taxonomy of the input data. In this dissertation, two real life use cases will be analyzed. The first is related to smart cities \cite{6924449},\cite{DBLP:conf/sebd/BaralisCCCGPG14} while the second will analyze network traffic logs \cite{apiletti20143}.

\subsection{Dissertation Plan}
This dissertation is organized in the following way. Chapter \ref{background} introduce the background related to frequent itemset mining and the distributed platforms involved. Most of all, it will motivate the problem statement and explain the challenges related to scalable implementations of new frequent itemset mining algorithms.
In Chapter \ref{survey} a thorough review of the most affirmed solutions will be introduced. The performance of the best-in-class implementation will be evaluated through the utilization of synthetic and real datasets, evidencing the current limitation of the academic state of the art.
Then, in Chapter \ref{pampa} an innovative distributed algorithm will be presented and evaluated, demonstrating its effectiveness in the context of high-dimensional pattern mining.
In Chapter \ref{nemico} a big data mining framework will be introduced and exploited to obtain a special type of frequent itemsets from network traffic and smart cities datasets.
Finally, Chapter \ref{thesis_conclusion} summarizes the main results we achieved and provides some future possible work directions. 



