This Chapter introduced PaMPa-HD,
a novel frequent closed itemset mining algorithm
able to efficiently parallelize the itemset extraction from extremely
high-dimensional datasets.
Experimental results show its good scalability
and its efficient performance in dealing with real-world datasets
characterized by up to 8 millions different items
and, above all, an average number of items per transaction
over hundreds of thousands,
on a small commodity cluster of 5 nodes.
PaMPa-HD outperforms state-of-the-art algorithms, by showing a better
scalability than all popular distributed approaches, such as PFP,
DistEclat and BigFIM.
%Further developments of the framework can be related to the introduction of new
%pruning rules in specific use cases. This pruning,
%so far related to the post processing phase, would avoid the processing of
%useless data.

Further developments of the algorithm can be related to the analysis of the trade-off between the benefits of the scalability and the ones related to the local state. 
In addition, future works could analyze the introduction of better load balancing mechanisms. The increasing $max\_exp$ parameter introduced by the self-tuning strategies leads to a degradation of the load balancing between the parallel tasks of the job. As shown in Table~\ref{load balance breast}, higher $max\_exp$ values decrease load balancing (i.e. only few tasks running), wasting the resources assigned to the tasks that are already complete.
Forcing the synchronization phase after a fixed period of time would limit the amount of time in which the resources are not completely exploited. From the algorithmic point of view, this is not a loss, since the tables are expanded in a depth-first fashion. The last tables, hence, are the ones with highest probability to be pruned. This future development, therefore, would analyze the choice of the \textit{time-out} which forces the synchronization phase.

%\textbf{\#Daniele @Fabio: non c'e' nessun altro futur work che ti viene in mente? solo pruning?
%Pietro diceva nei commenti di riprendere le open issue della scalabilita (vedi esperimento), magari possiamo mettere qualcosa su quello, tipo ridurre l'impatto del local state che limita la scalabilita...? Fabio: ci penso, tanto è per dire. Quello del local state secondo me è abbastanza insormontable e caratterizza tutti gli alg distribuiti. ma ci sta. La mia idea sarebbe di cercare  un modo per non fare la distribuzione per ramo ma per profondità. Lo scrivo io.}
%We plan to apply this approach in the network data analysis domain
%and to develop an Apache Spark implementation.
%We also plan to improve the expansion threshold selection:
%an auto-tuning mechanism based on the specific data distribution
%might further unleash the algorithm potential.
