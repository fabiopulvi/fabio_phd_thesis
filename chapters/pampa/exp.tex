In this section, we present a set of experiments to evaluate the performance of the proposed algorithm. 
Firstly, we asses the impact on performance of the maximum expansion threshold ($max\_exp$ ) parameter (Section~\ref{exp_fisso}). This phase is mandatory in order to tune-up the parameter configuration to compare the proposed approach with the state-of-the-art algorithms. Because the tuning of the parameter is not trivial, 
we discuss and experimentally evaluate some self-tuning
strategies to automatically set the $max\_exp$ parameter and improve the performance (Section~\ref{exp_strategies}). 

Next, we evaluate the speed of the proposed algorithm,
comparing it with the state-of-the-art distributed approaches 
(Section~\ref{running_time}). 
Finally, we experimentally analyze the impact of 
(i) the number of transactions of the input dataset (Section~\ref{number_rows}),
(ii) the number of parallel tasks (Section~\ref{scalability}), and
(iii) the communication costs and load balancing behavior (Section~\ref{communication_cost}).

Experiments have been performed on two real-world datasets.
The first is the PEMS-SF dataset~\cite{uci},
which describes the occupancy rate of different car lanes 
of San Francisco bay area freeways 
(15 months worth of daily data from the California Department of Transportation~\cite{pems}).
Each transaction represents the daily traffic rates of 963 lanes, sampled every 10 minutes.
It is characterized by 440 rows and 138,672 attributes (6 x 24 x 963), 
and it has been discretized in equi-width bins, each representing 0.1\% occupancy rate.

%Since PaMPa-HD is designed to cope with high-dimensional datasets 
%characterized by a small number of transactions, 
%we have used several down-sampled versions (in terms of number of rows) of the datasets 
%to measure the impact of the number of transactions on the performance of the algorithm.
As mentioned, PaMPa-HD design is focused on scaling up in terms of number of attributes, 
being able to cope with high-dimensional datasets. For this reason, we have used a 100-rows version of the PEMS-SF dataset for all the experiments. However, we have used the full dataset and several down-sampled versions (in terms of number of rows) to measure the impact of the number of transactions on the performance of the algorithm (Section~\ref{number_rows}).
%characterized by a small number of transactions, 
%we have used several down-sampled versions (in terms of number of rows) of the datasets 
%to measure the impact of the number of trfactions on the performance of the algorithm.

The second dataset is the Kent Ridge Breast Cancer~\cite{breast_cancer_dataset}, 
which contains gene expression data.
It is characterized by 97 rows that represent patient samples, 
and 24,482 attributes related to genes.
The attributes are numeric (integers and floating point).
Data have been discretized with an equal-depth partitioning
using 20 buckets (similarly to~\cite{Zaki_Carpenter}).
The discretized versions of the real datasets
are publicly available at http://dbdmg.polito.it/PaMPa-HD/.

%Because of their distribution and their discretizazion process, the Breast Cancer dataset is more sparse (low correlation among the dataset transactions) than the PEMS-SF dataset.
%The other two datasets were synthetically generated and tuned to simulate
%use cases characterized by extremely high-dimensional data,
%i.e., with massive numbers of features.
%Both datasets consists of 30 transactions.
%Dataset~\#1 has 1,000,000 different items
%and an average transaction length of 500,000 items,
%while
%Dataset~\#2 is 10 times larger, with 10,000,000 different items
%and an average transaction length of 5,000,000 items
%(see Table~\ref{datasets}).



\begin{table}[h!]
\begin{center}
\caption{Datasets}
\label{datasets}
\begin{tabular}{|c|c|c|c|}
\hline
	Dataset & Number of  & Number of & Number  \\
	 & transactions &different items & of items  \\
	  &  & &  per transaction  \\ \hline \hline

PEMS-SF    & 440& 8,685,087     & 138,672 \\
   %  Dataset      & (100 rows version)   &   (5,748,097)       &  \\ \hline
       Dataset      &  &          &  \\ \hline
     Kent Ridge Breast    & 97 & 489,640    & 24,492 \\
     Cancer Dataset      &    &            &  \\ \hline
%	Synthetic Dataset \#1 & 30 & 1,000,000  & 500,000\\ \hline
%	Synthetic Dataset \#2 & 30 & 10,000,000 & 5,000,000\\ \hline
\end{tabular}
\end{center}
\end{table}


PaMPa-HD is implemented in Java 1.7.0\_60 using the Hadoop MR API.
The experiments were performed on a cluster of 5 nodes running Cloudera
Distribution of Apache Hadoop (CDH5.3.1).
Each cluster node is a 2.67 GHz six-core Intel(R) Xeon(R) X5650 machine
with 32 Gbyte of main memory
running Ubuntu 12.04 server with the 3.5.0-23-generic kernel.


\subsection{Impact of the maximum expansion threshold}\label{exp_fisso}
In this section we analyze the impact of the maximum expansion threshold
($max\_exp$) parameter, which indicates the maximum number of nodes
to be explored before a preemptive stop of each distributed sub-process is forced.
This parameter, as already discussed in Section~\ref{Distributed implementation outline},
strongly affects the enumeration tree exploration,
forcing each parallel task to stop before completing the visit of its sub-tree
and send the partial results to the synchronization phase.
This approach allows the algorithm to globally apply
pruning rule 3 and reduce the search space.
Low values of $max\_exp$ threshold increase the load balancing,
because the global problem is split into simpler and less memory-demanding
sub-problems, and, above all, facilitate the global application of pruning rule 3,
hence a smaller subspace is searched.
However, higher values allow a more efficient execution,
by limiting the start and stop of distributed tasks
(similarly to the context switch penalty) and the synchronization overheads. Above all, higher values enhance the pruning effect of the state centralized memory.
%\textbf{(Considerare per tutti i grafici una possibile inversione di ordine (mettere prima gli esperimenti con dataset Breast Cancer e poi PEMS-SF), causata dalla `'poca'' bellezza degli esperimenti con strategy 1 per pems dataset. }
In order to assess the impact of the expansion threshold parameter, we have performed two sets of experiments. In the first one we perform the mining on the PEMS-SF (100 transactions) dataset with minsup = 10, by varying $max\_exp$ from 100 to 100,000,000.  The minsup value has been empirically selected to highlight the different performance related to different values (trivial mining would be overwhelmed by overhead costs of the MapReduce framework).
In Figure~\ref{pems_fixed} are shown the results in terms of execution time and number of iterations
(i.e., the number of jobs)\footnote{Please note that in all the experiments, for the sake of clarity, the confidence intervals (obtained after a sufficient number of executions and with  complementary level of significance of 95\%) are omitted from the graphs.}.
It is clear how the $max\_exp$ parameter can influence the performance, with wall-clock times that can be doubled with different configurations. The best performance in terms of execution time is achieved with a maximum
expansion threshold equal to 10,000 nodes. With lower values, the execution times are slightly longer, while there is an evident performance degradation with higher $max\_exp$ values.
This result highlights the importance of the synchronization phase.
Increasing the $max\_exp$ parameter makes the number of iterations decreasing,
but more useless tree branches are explored,
because pruning rule 3 is globally applied less frequently.
Lower values of  $max\_exp$, instead, raising the number of iterations, introduce a slight performance
degradation caused by iterations overheads.
%With very high values of  $max\_exp$, the running time and the number of
%iterations are stable because the bottleneck becomes the free available
%memory, and the synchronization job is
%automatically applied, independently of the value of  $max\_exp$.

\begin{figure}[!t]
\includegraphics[width=5in]{chapters/pampa/immagini_extension/pems_fixed.png}
\caption{Execution time and number of iterations for different $max\_exp$ values on PEMS-SF dataset with $minsup$=10.
}
\label{pems_fixed}
\end{figure}

\begin{figure}[!t]
\includegraphics[width=5in]{chapters/pampa/immagini_extension/breast_fixed.png}
\caption{Execution time and number of iterations for different $max\_exp$ values on Breast Cancer dataset with $minsup$=5.
}
\label{breast_fixed}
\end{figure}

The same experiment is repeated with the Breast Cancer dataset and a minsup value of 5. As shown in Figure~\ref{breast_fixed}, even in this case, the best performances are achieved with $max\_exp$ equal to 10,000. In this case, differences are more significant with lower $max\_exp$ values, although with a non-negligible performance degradation with higher values.

The value of $max\_exp$ impacts also the load balancing
of the distributed computation among different nodes.
With low values of $max\_exp$, each task explores a
smaller enumeration sub-tree, decreasing the size difference
among the sub-trees analyzed by different tasks,
thus improving the load balancing.
Table~\ref{load balance breast} reports the minimum and the maximum execution time of
the mining tasks executed in parallel for both the datasets and for two extreme values of $max\_exp$.
The load balance is better for the lowest value of $max\_exp$.



\begin{table}
\begin{center}
\caption{Load Balancing}
\label{load balance breast}
\begin{tabular}{ |c| c | c| c| c| }
\hline
							    &
\multicolumn{2}{|c|}{Task execution time}    & \multicolumn{2}{|c|}{Task execution time}      \\
 & \multicolumn{2}{|c|}{Breast Cancer}    & \multicolumn{2}{|c|}{PEMS-SF}      \\ \hline \hline
	Maximum expansion threshold &   Min          & Max    &   Min          & Max          \\ \hline
	100,000,000                 &    7 m                      & 2h 16m 17s &    44s                      & 2h 20m 28s
  \\ \hline
10                         &    6m 21s                      &        45m 16s  &   6s                      &        2m 24s
 \\ \hline
\end{tabular}
\end{center}
\end{table}





The $max\_exp$ choice has a non-negligible impact on the performances of the algorithm. However, as demonstrated by the curves in Figures~\ref{pems_fixed} and~\ref{breast_fixed}, it is very dependent on the use case and distribution of the data.
In the next subsection we introduce and motivate some tuning strategies related to $max\_exp$.

%
%This set of experiments has been performed on the Breast cancer dataset
%with Minsup 5, by varying $max\_exp$ from 100 to 100,000,000. This minsup value allowed to notice different
%wall-clock time for each expansion threshold value.
%Figure~\ref{exp_1} shows the results in terms of execution time and number of iterations
%(i.e., the number of jobs).
%The best performance in terms of execution time is achieved with a maximum
%expansion threshold equal to 10,000 nodes.
%With higher values, the number of iterations decreases,
%but more useless tree branches are explored,
%because pruning rule 3 is globally applied less frequently.
%Lower values of  $max\_exp$, instead, introduce a performance
%degradation caused by the higher number of iterations
%and the synchronization phase overheads.
%With very high values of  $max\_exp$, the running time and the number of
%iterations are stable because the bottleneck becomes the free available
%memory, and the synchronization job is
%automatically applied, independently of the value of  $max\_exp$.
%The tuning of $max\_exp$ is strictly related to the data distribution:
%in general, the easier the mining task, the fewer the benefits of having
%many iterations.
%
%The value of $max\_exp$ impacts also the load balancing
%of the distributed computation among different nodes.
%With low values of $max\_exp$, each task explores a
%smaller enumeration sub-tree, decreasing the size difference
%among the sub-trees analyzed by different tasks,
%thus improving the load balancing.
%Table~\ref{load balance} reports the minimum and the maximum execution time of
%the mining tasks executed in parallel for two extreme values of $max\_exp$.
%The load balance is better for the lowest value of $max\_exp$.
%\textbf{tutto vecchio fin qui}
%
%
%\begin{figure}[!t]
%\includegraphics[width=5in]{grafo_exp2.png}
%\caption{Execution time and number of iterations for different $max\_exp$ values on Breast Cancer dataset with $minsup$=5.
%}
%\label{exp_1}
%\end{figure}
%
%
%\begin{table}
%\begin{center}
%\caption{Load Balancing}
%\label{load balance}
%\begin{tabular}{ |c| c | c| }
%\hline
%							    &
%\multicolumn{2}{|c|}{Task execution time}          \\ \hline
%	Maximum expansion threshold &   Min          & Max            \\ \hline
%	100,000,000                 &    4s                      & 1h 54m 33s
%  \\ \hline
%100,000                 &    4s                      & 1h 2m 32s
%  \\ \hline
%	10000                         &    4s                      &        23m 50s
%\\ \hline
%100                         &    4s                      &        53s
% \\ \hline
%\end{tabular}
%\end{center}
%\end{table}


\subsection{Self-tuning strategies}\label{exp_strategies}

This section introduces some heuristic strategies related to the $max\_exp$ parameter.
%The aim of this experiment is to identify a heuristic technique 
%which is able to deliver good performances 
%without requiring the user to manually tune the $max\_exp$ parameter.
The aim of this experiment is to identify a heuristic technique 
able to improve the performances of the algorithm and easily configure the algorithm parameter. The heuristic consists in the automatic modification, inside the mining process, of the $max\_exp$ parameter,
without requiring the user to manually tune it.
To introduce the techniques, we provide motivations behind their design in the following.
Because of the enumeration tree structure, the first tables of the tree are the most populated. Each node, in fact, is generated from its parent node as a projection of the parent transposed table on a tid.
In addition, the first nodes are, in the average, the ones generating more sub-branches. By construction, their transposed table tidlists are, by definition, longer than the ones of their children nodes. This increases the probability that the table could be expanded.
For these reasons, the tables of the initial mining phase are the ones requiring more resources and time to be processed.
On the other hand, the number of nodes to be processed by each local Carpenter iteration tends to increase with the number of iterations. Still, this factor is mitigated by (i) the decreasing size of the tables and (ii) the eventual end of some branches expansion (i.e. when there are not more tids in the node transposed table).
These reasons motivated us to introduce four strategies (Table~\ref{table_strategies}) that assume a maximum expansion threshold which is increased with the number of iterations. These strategies start with very low values in the initial iterations  (i.e. when the nodes require a longer processing time) and increase $max\_exp$ during the mining phases.

\textit{Strategy \#1} is the simplest: $max\_exp$ is increased with a factor of $X$ at each iteration. For instance, if $max\_exp$ is set to 10, and $X$ is set to 100 at the second iteration it is raised to 1000 and so on.
In addition to this straightforward approach, we leverage information about (i) the execution time of each iteration and the (ii) pruning effect (i.e. the percentage of transposed tables / nodes that are pruned in the synchronization job).

The aim of the \textit{strategy \#2} is balancing the execution times among the iterations, trying to avoid a set of very short final jobs.
Specifically, \textit{strategy \#2} increases, at each iteration, the $max\_exp$ parameter with a factor of  $X^{T_{old} / T_{new}}$, where $T_{new}$ and $T_{old}$ are, respectively, the execution times of the previous two jobs.

For \textit{strategy \#3}, we analyzed the pruning impact of the synchronization phase (i.e. the percentage of pruned table due to redundancy). An increasing percentage of pruned tables means that there are a lot of useless tables that are generated. Hence, this could suggest to limit the growth of the $max\_exp$ parameter. However, the pruning effect is an information which cannot be easily interpreted. In fact, an increasing trend of the pruning percentage is also normal, since the number of nodes that are processed increases exponentially. Given that our intuition is to rise the  $max\_exp$ among the iterations, in \textit{strategy \#3}, we increase the $max\_exp$ parameter with a factor $X^{Pr_{old} / Pr_{new}}$, given $Pr_{new}$ and $Pr_{old}$ the relative number of pruned tables in the previous two jobs. In this way, when the pruning impact increases ($Pr_{new}\ge Pr_{old}$), the growth of $max\_exp$ is slowed.
%For \textit{strategy \#3}, we take into account the relative number of pruned tables. Indeed, this value cannot be easily interpreted. An increasing pruning percentage means that there are a lot of tables that are generated uselessly. However, an increasing trend is also normal, since the number of nodes that are processed increases exponentially. Given that our intuition is to rise the  $max\_exp$ among the iterations, in \textit{strategy \#3}, we increase the $max\_exp$ parameter with a factor $X^{Pr_{old} / Pr_{new}}$, given $Pr_{new}$ and  $Pr_{old}$ the relative number of pruned tables in the previous two jobs.

Finally, \textit{strategy \#4} is inspired by the congestion control of TCP/IP (a data transmission protocol used by many Internet applications~\cite{Jacobson:1988:CAC:52325.52356}). This strategy, called ``Slow Start'', assumes two ways for growing the window size (i.e. the number of packets that are sent without congestion issues): an exponential one and a linear one. In the first phase, the window size is increased exponentially until it reaches a threshold (``ssthresh'', which is calculated from some empirical parameters such as Round Trip Time value). From that moment, the growth of the window becomes linear, until a data loss occurs. In \textit{strategy \#4}, the $max\_exp$ is handled like the congestion window size.

In our case, we just inherit the two growth factor approach. Therefore, our ``slow start'' strategy consists in increasing the $max\_exp$ of a factor of $X$ ($X\geq10$) until the last iteration reaches an execution time greater than a given threshold. After that, the growth is more stable, increasing the parameter of a factor of 10. Please note that we have fixed the threshold to the execution time of the first two jobs (Job 1 and Job 2). These jobs, for the architecture of our algorithm, consists of the very first Carpenter iteration. They are quite different than the others since the first Mapper phase builds the initial projected transposed tables (first level of the tree) from the input file. This choice is consistent with our initial aim, that is to normalize the execution times of the last iterations which are often shorter than the first ones.
%\textbf{Fabio \& Paolo:Non siamo sicuri che convenga inserire questa parte sul time out. Michiardi: I guess it is ok: mechanisms like speculative execution work similarly, hence to me the approach is not shocking. @TANIA: allora eliminiamo e magari lo mettiamo come idea per i future works}.
%The increasing $max\_exp$ value introduced by the described strategies, however, leads to a degradation of the load balancing between the parallel tasks of the job. To limit this issue, we have introduced a timeout of 1 hour. After that, all the tasks will be forced to run the synchronization job. From the algorithmic point of view, this is not a loss, since the the tables are expanded in a depth-first fashion. The last tables, hence, are the ones with the highest probability to be pruned. Although, in this way, we are limiting to 1 hour the amount of time in which we are not completely exploiting the resources of the commodity cluster (i.e. only few very long tasks running). A value of 1 hour has been empirically proved to be a good trade-of between load balancing and a good leveraging of the centralized memory pruning.

\begin{table}
\begin{center}
\caption{Strategies}
\label{table_strategies}
\begin{tabular}{|c|c|c|}
\hline
Strategy \#1($X$)  & Constant growth& Increasing at each iteration      \\
            &of the parameter       & with a factor of $X$               \\ \hline

Strategy \#2($X$) & Job balancing via & Increasing at each iteration with \\
       & execution time analysis           & a factor of $X^{T_{old} / T_{new}}$                   \\ \hline


Strategy \#3($X$) & Job balancing via & Increasing at each iteration with \\
              &pruning impact analysis     & a factor of $X^{Pr_{old} / Pr_{new}}$                    \\ \hline
                   Strategy \#4  &Slow start      & Fast increase with a    factor of                  \\
 &    & $X$, slow increase with a factor of $10$                     \\ \hline

\end{tabular}
\end{center}
\end{table}


\begin{table}
\begin{center}
\caption{Strategies performance}
\label{strategies_perf}
\begin{tabular}{ |c| c | c| }
\hline
 Strategies & PEMS-SF& Breast Cancer   \\ \hline \hline
  Strategy \#1 &-6.48\%   &    -19.03\% \\
   &
 (X = 10)  &   (X = 100,000 )   \\ \hline
  Strategy \#2 & --3.73\% &  -0.02\%   \\
      & (X = 1,000)&  (X = 10,000 )    \\ \hline
  Strategy \#3 & -4.42\%  & +1.59\%   \\
      &  (X = 100)&  (X = 100)   \\ \hline
   Strategy \#4 & +9.39\%
 &  -16.17\%   \\
      &(X = 100) & (X = 1,000 )    \\ \hline
\end{tabular}
\end{center}
\end{table}

\textit{Strategy \#1} is the one achieving the best performances for both the datasets. Table~\ref{strategies_perf} reports the best performance for each strategy, in terms of relative performance difference with the best results obtained with a fixed $max\_exp$ parameter. For PEMS-SF dataset, even \textit{strategies \#2 and \#3} are able to achieve positive gains. For Breast Cancer dataset \textit{strategy \#1} is the best, followed by \textit{strategy \#4}: these are the only ones achieving significant positive gain over the fixed $max\_exp$ approach. All the strategies are evaluated with $X$ from 10 to 100,000.

As shown in Table~\ref{strategies_perf}, the results among the datasets are quite different. It is clear that Breast Cancer data distribution better fits the fast growth of the parameter, as shown by the better results with respect to the PEMS-SF dataset. The benefits of the growth of the $max\_exp$ parameter with PEMS-SF dataset are, indeed, limited.  The reason behind this behavior is related to the data distribution. With PEMS-SF dataset, the mining process generates more intermediate results. In this scenario, a more frequent synchronization phase delivers more benefits with respect to the Breast Cancer dataset.  
The analysis is confirmed also by the best values of $X$ with the two datasets. Breast Cancer experiments are characterized by a higher increase factor than the ones related to PEMS-SF dataset.


Since the best performance is achieved with values of 10 and 100,000 respectively for PEMS-SF and Breast Cancer datasets (improvement of almost 6\% and 20\%), we will use this configuration for the experiments comparing PaMPa-HD with other distributed approaches. 
%The difference may be caused by the characteristics of the dataset: evidently, PEMS-SF dataset benefits of more synchronization phases.
%
%\textbf{Fabio: queste figure sono indispensabili? Michiardi: in my opinion either: i) we omit them and only report \# in the text ii) we put a table. Fabio: Se approvate, le eliminerei e specifico la percentuale vincente nel testo, visto che la figura 9 è pessima e con quel picco cosi' alto potrebbe suscitare domande scomode. In questo modo si elimina anche il dubbio se invertire gli esperimenti coi due dataset come suggeriva Paolo.
%\#Daniele @Fabio: per me ok eliminare, propongo tabella come Pietro @TANIA: secondo me con la tabella si vede ancora di piu che i risultati a volte sono molto negativi, tanto c'e gia' la tabella generica. con paolo proponiamo di lasciarlo solo nel testo.}



%Finally, in all the experiments \textbf{citare quelli di prima}, we have notices some very short execution times (less than a minute) in the last mining iterations. This surely increases the impact of MapReduce job handling overhead on the global performances.
%All these things, motivated us to introduce some strategies that assumes a maximum expansion threshold that is increased with the number of iterations.
%All these things, together with very short last iterations (with an increasing MapReduce job overhead), motivated us to test some strategies that assumes a maximum expansion threshold that is increased with the iterations.

%The strategy \#1 is straightforward: the $max\_exp$ is increased with a factor of $X$ at each iteration. For instance, if the $max\_exp$ is set to 10, and $X$ is set to 100 at the second iteration it is raised to 1000 and so on.
%
%Alternatively, we wanted to achieve a balanced growth of the $max\_exp$ parameter. We want to balance the load of the iterations, but, on the other hand, we want to avoid an overgrowth of the parameter and, therefore, of the execution times of the last iterations. As already said, in fact, increasing the $max\_exp$ parameter decreases the impact of the synchronization job pruning.
%In order to monintor the growth, we introduced a set of techniques based on the execution times and a strategy which monitors the impact of the synchronization jobs.
%%In order to monitor this growth, we firstly thought about an index measuring the effectiveness of the pruning in terms of closed or tables pruned in the synchronization job. However, the effectiveness of the pruning cannot be easily interpreted. An increasing pruning effect means that there are a lot of tables that are generated uselessly. However, an increasing pruning effect is also normal since the number of nodes that are processed continues to increase. \textbf{inserire degli esperimenti in cui faccio vedere che usare il pruning effect riduce le performance}.
%In strategies \#2 and \#3, we took into exam the execution time of the iterations.
%Spefically, strategy \#2 consists in increasing, at each iteration, the $max\_exp$ parameter with a factor of  $X^{T_{old} / T_{new}}$, given $T_{new}$ and  $T_{old}$ the execution time of the previous two jobs. The motivation is to balance the growth of the parameter in order to achieve a stable execution times among the iterations.
%Strategy \#3, instead, is inspired by the congestion control of TCP/IP (a data transmission protocol used by many Internet applications~\cite{}). Precisely, the $max\_exp$ is handled like the congestion window size (i.e. the number of packets that are sent without congestion issues).
%This strategy, called ``Slow Start'', assumes two types of growing of the window size: an exponential one and a linear one. In the first phase, the window size is increased exponentially until it reaches a threshold (``ssthresh'', which is calculated empirically from RTT and other values). From that moment, the growth of the window becomes linear, until a data loss occurs.
%In our case, we are not interested we just inherit the two growth factors. Therefore, our ``slow start'' strategy consists in increasing the $max\_exp$ of a factor of $X$ until the last iteration reaches an execution time greater than a given threshold. After that, the growth is more stable, increasing the parameter of a factor of 10 (for this reason $X$>10).
%We have fixed the threshold to the execution time of the first two jobs (Job 1 and Job 2). These jobs, for the architecture of our algorithm, consists of the very first Carpenter iteration. They are quite different than the others since the first Mapper phase has to build the initial projected transposed tables (first level of the tree) from the input file.
%We have selected the execution time of the first iteration since it is consistent with our initial aim,
%that is to normalize the execution times of the last iterations which are often shorter than the first ones.
%
%The last strategy, \#4, is based on the effectiveness of the pruning in terms of closed or tables pruned in the synchronization job. Indeed, the measure of the relative number of tables that are pruned cannot be easily interpreted. An increasing pruning percentage means that there are a lot of tables that are generated uselessly. However, an increasing trend is also normal, since the number of nodes that are processed continues to increases. Given that our intuition is to rise the  $max\_exp$ among the iterations, in strategy \#4, we increase the $max\_exp$ parameter with a factor $X^{Pr_{old} / Pr_{new}}$, given $Pr_{new}$ and  $Pr_{old}$ the relative number of pruned tables in the previous two jobs.
%%Even if strategy \#4 can be meant as the dual of strategy \#2, with the usage of the pruning ratios instead of the different execution times, we decided not to do the same for strategy \#3. In this case, we could not identify a proper threshold to be used for changing the growth factor of $max\_exp$. In strategy \#3, we have used the first iteration because the initial motivation of this empirical study is to stabilize the execution times. In this case, we don't consider the initial pruning ratio as a valid threshold: it is computed after only few tables and, in the worst case, can be 0.
%
%In our experiment, we have fixed the initial  $max\_exp$ value to 10. This very low value is motivated by the nature of our strategies, which consist of a balanced increasing of the parameter.
%We applied the strategies (resumed in Table~\ref{table_strategies}) to the same experiments of Figures~\ref{breast_fixed} and ~\ref{pems_fixed}, in order to compare execution times the ones obtained with the optimum choice of $max\_exp$.
%In oder to assess the impact of the $X$ parameter, we have used values from 10 to 10,000 (except for strategy3 for which we have used values from 100 to 10,000).
%
%The result of the application of the techniques to the PEMS-SF dataset are shown in Figures~\ref{pems_strategy1},~\ref{pems_strategy2},~\ref{pems_strategy3},~\ref{pems_strategy4}, which represent the relative execution time gain with respect to the best execution time obtained with the fixed $max\_exp$ of 10,000. In Figure~\ref{pems_strategy_best} we have grouped the best configuration for each strategy in order to easily compare them.
%It is clear how the almost all the strategies improve the performance of the algorithm. Only Strategy3 showed to be slower. Strategy1(1,000), Strategy2(10,000) and Strategy4(1,000) achieve a very similar speedup.
%
%We have repeated the experiment with Breast Cancer dataset and a minsup value of 6 (Figures~\ref{breast_strategy1},~\ref{breast_strategy2},~\ref{breast_strategy3},~\ref{breast_strategy4}). We have raised $X$ from 10 to 10,000. Only with Strategy2, as shown in Figure~\ref{breast_strategy2}, we have raised it to 100,000 , because the experiments could suggest a decreasing execution times trend, but it was not the case.
%As before, the results are grouped in Figure~\ref{breast_strategy_best}.
%In this case, all the strategies achieve a positive speedup with respect to the best execution time obtained with a fixed $max\_exp$ parameter. In addition, the same configurations for each strategy demonstrated to be most performant in both the experiments.
%The results obtained with the adoption of these strategies are very similar, and the differences are negligible.
%
%
%
%
%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/pems_strategy1.png}
%\caption{Relative gains on Pems-SF dataset with $minsup$=10, Strategy1 and different $X$ values.
%}
%\label{pems_strategy1}
%\end{figure}

%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/pems_strategy2.png}
%\caption{Relative gains on Pems-SF dataset with $minsup$=50, Strategy2 and different $X$ values.
%}
%\label{pems_strategy2}
%\end{figure}
%
%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/pems_strategy3.png}
%\caption{Relative gains on Pems-SF dataset with $minsup$=50, Strategy3 and different $X$ values.
%}
%\label{pems_strategy3}
%\end{figure}
%
%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/pems_strategy4.png}
%\caption{Relative gains on Pems-SF dataset with $minsup$=50, Strategy4 and different $X$ values.
%}
%\label{pems_strategy4}
%\end{figure}
%
%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/pems_strategy_best.png}
%\caption{Relative gains of the best configuration for each strategy, on Pems-SF dataset with $minsup$=50.
%}
%\label{pems_strategy_best}
%\end{figure}
%
%
%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/breast_strategy1.png}
%\caption{Relative gains on Breast Cancer dataset with $minsup$=5, Strategy1 and different $X$ values.
%}
%\label{breast_strategy1}
%\end{figure}

%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/breast_strategy2.png}
%\caption{Relative gains on Breast Cancer dataset with $minsup$=6, Strategy2 and different $X$ values.
%}
%\label{breast_strategy2}
%\end{figure}
%
%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/breast_strategy3.png}
%\caption{Relative gains on Breast Cancer dataset with $minsup$=6, Strategy3 and different $X$ values.
%}
%\label{breast_strategy3}
%\end{figure}
%
%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/breast_strategy4.png}
%\caption{Relative gains on Breast Cancer dataset with $minsup$=6, Strategy4 and different $X$ values.
%}
%\label{breast_strategy4}
%\end{figure}
%
%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/breast_strategy_best.png}
%\caption{Relative gains of the best configuration for each strategy, on Pems Cancer dataset with $minsup$=6.
%}
%\label{breast_strategy_best}
%\end{figure}
%


\subsection{Execution time}\label{running_time}
Here we analyze the efficiency of PaMPa-HD by comparing it 
with three distributed state-of-the-art frequent itemset mining algorithms:

\begin{enumerate}

\item Parallel FP-growth~\cite{pfpgrowth} 
available in Mahout 0.9~\cite{mahout2}, 
based on the FP-growth algorithm~\cite{Han00} \footnote{The Spark MLlib~\cite{citeulike:13636750} implementation has not been included into the evaluation because it extracts all the frequent itemsets and not just the closed ones.}

\item DistEclat~\cite{bigfim}, based on the Eclat algorithm~\cite{Zaki97newalgorithms}

\item BigFIM~\cite{bigfim}, inspired from the Apriori~\cite{Agr94} and DistEclat
\end{enumerate}

This set of algorithms represents the most cited implementations 
of frequent itemset mining distributed algorithms. 
All of them are Hadoop-based and are designed to extract 
the frequent closed itemsets 
(DistEclat and BigFIM actually extract a superset of the frequent closed itemsets).
The parallel implementation of these algorithms has been aimed to scale in the number of transactions of the input dataset. Therefore, they are not specifically developed to deal with
high-dimensional datasets as PaMPa-HD.
The algorithms have been already discussed in detail in Section~\ref{survey_algorithms}.

Even in this case, the frameworks are compared over the two real dataset (PEMS-SF and Breast Cancer datasets) The experiments are aimed to analyze the performance of PaMPa-HD with respect to the best-in-class approaches in high-dimensional use-cases. 
%The comparison with Carpenter aims to show the ability of PaMPa-HD
%to address datasets that are not manageable by means of a
%centralized approach.
%The comparison with the Hadoop implementation of PFP is
%a reference benchmark since it represents
%the most scalable state-of-the-art approach for closed itemset
%mining, even if PFP was not specifically developed to deal with
%high-dimensional datasets as PaMPa-HD.
The first set of experiments has been performed with the 100-rows version PEMS-SF dataset~\cite{uci} and minsup values 35 to 5.\footnote{The algorithms parameters, which will be introduced in Section~\ref{experimental}, has been set in the following manner. PFP has been set to obtain all the closed itemsets; the prefix length of the first phase of BigFIM and DistEclat, instead, has been set to 3, as suggested by the original paper~\cite{bigfim}, when possible (i.e. when there were enough 3-itemsets to execute also the second phase of the mining).}

As shown in Figure~\ref{pems_confronto}, in which minsup axis is reversed to improve readability, PaMPa-HD is the only algorithm able to complete all the mining task to a minsup value of 5 rows or 5\%. All the approaches show similar behaviors with high minsup values (from 30 to 35).
With a minsup of 25, PFP shows a strong performance degradation, being not able to complete the mining.
In a similar way, BigFIM shows a performance degradation with a minsup of 20, running out of memory with a minsup of 15.
DistEclat, instead, shows very interesting execution time until running out of memory with a minsup of 10.
PaMPa-HD, even if slower than DistEclat with minsup values from 25 to 15, is able to complete all the tasks.


\begin{figure}[!t]
\includegraphics[width=5in]{chapters/pampa/immagini_extension/pems_confronto.png}
\caption{Execution time for different Minsup values on the PEMS-SF dataset (100-rows).}
\label{pems_confronto}
\end{figure}

\begin{figure}[!t]
\includegraphics[width=5in]{chapters/pampa/immagini_extension/breast_confronto.png}
\caption{Execution time for different Minsup values on the Breast Cancer dataset.}
\label{breast_confronto}
\end{figure}
The second set of experiments are performed with the Breast
Cancer dataset~\cite{breast_cancer_dataset}.
As reported in Figure~\ref{breast_confronto} (Even in this case, minsup axis is reversed to improve readability, the minsup is absolute), PaMPa-HD is the most reliable and fast approach.
This time, BigFIM is not able to cope even with the highest minsup values, while PFP shows very slow execution times and runs out of memory with a minsup value of 6.
DistEclat is able to achieve good performances but is always slower than PaMPA-HD (with a minsup value equal to 4, it is not able to complete the mining within several days of computation).
From these results, we have seen how traditional best-in-class approaches such as BigFIM, DistEclat and PFP are not suitable for high-dimensional datasets. They are slow and/or not reliable when coping with the curse of dimensionality. PaMPa-HD, instead, demonstrated to be most suitable approach with datasets characterized by a high number of items and a small number of rows.
After the comparison with the state of the art distributed frequent itemset mining algorithms, the next subsections will experimentally analyze the behavior of PaMPa-HD with respect to the number of transactions, number of independent tasks, communication costs and load balancing.
%Because of the algorithm design of our approach, we already know that it is very sensitive with respect to the transactions of the input dataset. It will be interesting to evaluate the impact of the number of independent tasks. This issue is not trivial because adding a task to the computation would not only delivers more resources such as memory or CPU. An additional task leads to split the chunk of the enumeration tree that is explored

%\begin{figure}[!t]
%\includegraphics[width=5in]{chapters/pampa/immagini_extension/pems_confronto_200.png}
%\caption{Execution time for different Minsup values on the PEMS-SF dataset (200-rows).}
%\label{pems_confronto_200}
%\end{figure}

\subsection{Impact of the number of transactions}\label{number_rows}
This set of experiments measures the impact of the number of transactions on PaMPa-HD performances. To this aim, the PEMS-SF datasets will be used in three versions (100-rows, 200-rows and full).
The algorithm is very sensitive to this factor: the reasons are related to its inner structure. In fact, the enumeration tree, for construction, is strongly affected by the number of rows. A higher number of rows leads to:
\begin{enumerate}
\item A higher number of branches. As shown in the example in Figure~\ref{running_1}, from the root of the tree, it is generated a new branch for each tid (transaction-id) of the dataset.
\item Longer and wider branches. Since each branch explores its research subspace in a depth-first order, exploring any combination of tids, each branch would result with a greater number of sub-levels (longer) and a greater number of sub-branches (wider)
\end{enumerate}

Therefore, the mining processes related to the 100-rows version and to the 200-rows or the full version of PEMS-SF dataset are strongly different. With a number of rows incremented by, respectively, 200\% and more than 400\%, the mining of the augmented versions of PEMS-SF dataset is very challenging for the enumeration-tree based PaMPa-HD.
%The results in Figure~\ref{pems_confronto_200} and in Figuretoadd confirm these difficulties. With the same range of relative minsup values, BigFIM and PFP show a similar performance with the ones with the reduced version of the dataset (Figure~\ref{pems_confronto}). Their capacity to complete the mining is slightly and gradually reduced by some minsup values points.
%DistEclat, the algorithm which showed the most interesting performances among the ones not designed to fit high-dimensional use cases, shows a very stable behavior for all the mining tasks of the experiments in Figure~\ref{pems_confronto_200}. In the experiments with the full version of the dataset, interestingly, is not able to complete the mine even with a minsup of 50\%.
%PaMPa-HD shows a very strong performance degradation already in the experiments with 200-rows dataset, due to the higher number of transaction. Given $n$ the number of rows of the dataset, the size of the enumeration tree is proportional to $n^2$ \textbf{(che ne pensate?)} (while the approaches used by DistEclat, BigFIM and PFP are sensitive to the number of different items).
The performance degradation is resumed in Figure~\ref{pampa_pems_confronto}, where, for instance, with a minsup of 35\%, the execution times related to the 100-rows and the full version of the PEMS-SF dataset differ of almost two orders of magnitude.

The behavior and the difficulties of PaMPa-HD with datasets with an incremental number of rows, is, unfortunately, predictable. This algorithmic problem represents a challenging and interesting open issues for further developments.
\begin{figure}[!t]
\includegraphics[width=5in]{chapters/pampa/immagini_extension/pampa_pems_confronto.png}
\caption{Execution times for different versions of PEMS-SF for PaMPa-HD.}
\label{pampa_pems_confronto}
\end{figure}





\subsection{Impact of the number of nodes}\label{scalability}
The impact of the number of independent tasks involved in the algorithm execution is a non-trivial issue. 
Adding a task to the computation would not only deliver more resources such as memory or CPU, 
but it also leads to split the chunk of the enumeration tree that is explored by each task. 
On the one hand, this means to reduce the search space to explore, lightening the task load. 
On the other hand, this reduces the state centralized memory and the impact of the related pruning. 
It can be interpreted as a trade-off between the benefits of the parallelism against the state.
In Figure~\ref{scalability_img_pems} and Figure~\ref{scalability_img_breast}, it is reported the behavior of PaMPa-HD with a mining process on the datasets PEMS-SF and Breast Cancer. The minsup values, respectively of 20 and 6, have been chosen in order to highlight the performance differences among the different degree of parallelism and datasets.
Interestingly, the mining on PEMS-SF dataset is less sensitive to the number of reducers, with an execution time that is just halved when the independent tasks included in the computation pass from 1 to 17. The experiment of Breast Cancer instead, Figure~\ref{scalability_img_breast}, shows a stronger performance gain.
As before, the behavior is related to the dataset data distribution which causes the PEMS-SF mining process generating more intermediate tables.
In this case, the advantages related to additional independent nodes into the mining 
is mitigated by the loss of state in the local pruning phase inside the nodes. 
With additional nodes, each node is pushed to a smaller exploration of the search space, 
decreasing the effectiveness of the local pruning.
%PEMS-SF dataset, as discussed in Subsection~\ref{exp_strategies}, mitigates the lack of additional independent nodes with a more aggressive local pruning phase inside the nodes.
These specific results recall a very popular open issue in distributed environments. 
In problems characterized by any kind of ''state'' benefit 
(in this case, the local pruning inside the tasks), 
a higher degree of parallelism does not lead to better performance a priori.
%\textbf{(Riformulare meglio, questo apre questioni interessanti sulla questione di determinare il grado di parallelismo quando si deve lanciare un algoritmo. Finora c'e' stato il forte constraint dei cluster fisici, ora col cloud tutti possono scegliere piu' liberamente. Quindi e' davvero vero che più grande e' il parallelismo migliori saranno le performance? Riprendere il tutto anche nelle conclusioni nelle open issues)}.

\begin{figure}[!t]
\includegraphics[width=5in]{chapters/pampa/immagini_extension/scalability_pems.png}
\caption{Execution times for PEMS-SF dataset with different number of parallel tasks.}
\label{scalability_img_pems}
\end{figure}

\begin{figure}[!t]
\includegraphics[width=5in]{chapters/pampa/immagini_extension/scalability_breast.png}
\caption{Execution times for Breast Cancer dataset with different number of parallel tasks.}
\label{scalability_img_breast}
\end{figure}


\subsection{Load Balancing and communication costs}\label{communication_cost}
The last analyses are related to the load balancing and the communication costs of the algorithm. These issues represent very important factor in such a distributed environment. Communication costs are among the main bottlenecks for the performance of parallel  algorithms~\cite{Sarma:2013:ULB:2535570.2488334}.
A bad-balanced load among the independent tasks leads to few long tasks that block the whole job.

PaMPa-HD, being based on the Carpenter algorithm, mainly consists on the exploration of an enumeration tree. The basic idea behind the parallelization is to explore the main branches of the tree independently within parallel tasks (Figure~\ref{running_2}). For this reason, each task needs the information (i.e. transposed tables) related to its branch expansion.
The ideal behavior of a distributed algorithm would be to distribute the least amount of data, avoiding redundant informations as much as possible. The reason is that network communications are very costly in a Big Data scenario.
Unfortunately, the structure of the enumeration tree of PaMPa-HD assumes that some pieces of data of the initial dataset is sent to more than one task. For instance, some data related to nodes $TT|_{2}$ and $TT|_{3}$ are the same, because from node $TT|_{2}$ will be generated the node $TT|_{2, 3}$. This is an issue related to the inner structure of the algorithm and a full independence of the initial data for each branch cannot be reached.

In addition, the architecture of the algorithm, with its synchronization phase, increases the I/O costs. In order to prune some useless tables and improve the performance, the mining process is divided in more phases writing the partial results into HDFS.
However, as we have already seen when studying the impact of $max\_exp$ (Figure~\ref{pems_fixed} and Figure~\ref{breast_fixed}), in some cases additional synchronization phases lead to better execution times, despite their related overhead.

In Figure~\ref{comm_cost_pems} and~\ref{comm_cost_breast}, the communication cost during a mining process is reported. The spikes are related to the shuffle phases, in which the redundant tables and closed itemsets are removed.
The flat part of the curve between the spikes is longer in the case of the Breast Cancer dataset because of the adopted strategy. Its mining has been executed with a more aggressive increasing of the $max\_exp$ parameter (steps of 10 for PEMS-SF dataset, 10,000 for Breast Cancer dataset), which leads to a very long period without synchronization phases.


\begin{figure}[!t]
\includegraphics[width=5in]{chapters/pampa/immagini_extension/comm_cost_pems.png}
\caption{Received and sent data in the commodity cluster network during PEMS-SF dataset mining, minsup=20.}
\label{comm_cost_pems}
\end{figure}

\begin{figure}[!t]
\includegraphics[width=5in]{chapters/pampa/immagini_extension/comm_cost_breast.png}
\caption{Received and sent data in the commodity cluster network during Breast Cancer dataset mining, minsup=6.}
\label{comm_cost_breast}
\end{figure}

The load balancing is evaluated by comparing the execution time of the fastest and slowest tasks related to the iteration job in which this difference is strongest. The most unbalanced phase of the job is, not surprisingly, the mapper phase of the Job 3. This job is iterated until the mining is complete and it is the one more affected by the increase of the $max\_exp$ parameter (iterations characterized by high $max\_exp$ value are likely characterized by long and unbalanced task).
\begin{table}
\begin{center}
\caption{Load Balancing}
\label{load balance final}
\begin{tabular}{ | c | c| c| }
\hline
	Dataset						    & Slowest Task & Fastest Task  \\
							    &  Execution time &  Execution time \\ \hline \hline
	PEMS-SF & 3mins 58 sec & 3mins 37sec \\ \hline
	Breast Cancer & 20mins 33sec & 8mins 42sec\\ \hline
%\multicolumn{2}{|c|}{Task execution time}    & \multicolumn{2}{|c|}{Task execution time}      \\
% & \multicolumn{2}{|c|}{Breast Cancer}    & \multicolumn{2}{|c|}{PEMS-SF}      \\ \hline
%	Maximum expansion threshold &   Min          & Max    &   Min          & Max          \\ \hline
%	100,000,000                 &    7 m                      & 2h 16m 17s &    44s                      & 2h 20m 28s
%  \\ \hline
%10                         &    6m 21s                      &        45m 16s  &   6s                      &        2m 24s
% \\ \hline
\end{tabular}
\end{center}
\end{table}
The difference among the fastest and the slowest mapper is shown in Table~\ref{load balance final}. It is clear that the mining on PEMS-SF dataset is more balanced among the independent tasks. Even in this case, the reason is the different increment value in the Strategy \#1 (10 for PEMS-SF dataset, 10,000 for Breast Cancer dataset). A slower $max\_exp$ increasing leads to more balanced tasks.


%The difference among the fastest and the slowest mapper, as shown by Table~\ref{load balance final}, is not negligible. However, for both the datasets mining, the most unbalanced job is in a phase of the global mining in which there are a lot of input data, which correspond to a number of HDFS chunk greater than the number of mapper. Therefore, even if a long-tailed job does happen, the commodity cluster resources are not completely wasted because new mappers are scheduled as soon as the first ones are processed. This mitigates the load unbalance and the resource lose.




%This section addresses the scalability of PaMPa-HD with respect to the
%number of reducers, since the most heavy operations (i.e.,
%the execution of the ``local'' Carpenter) are executed by the reducers.
%The number of reducers varies from 1 to 18,
%which is the maximum number of tasks that can be run simultaneously
%in the commodity cluster at our disposal.
%The Breast cancer dataset and a minimum absolute
%support threshold equal to 6 have been used.
%As shown in figure~\ref{scalability},
%the increase of the number of reducers has
%a positive impact on the execution time
%when the number of reducers is less than 10.
%The marginal benefits decrease as more reducers are added to the computation.
%Therefore, with more than 10 reducers, the benefits of the load distribution
%are compensated by the decreased effectiveness of the local pruning (i.e., the
%more the load is distributed, the less effective the local pruning is).
% %As already mentioned, the behavior is strictly connected with the considered
% dataset.

%
%\begin{figure}[!t]
%\includegraphics[width=5in]{grafo_scalability.png}
%\caption{Execution time with different numbers of reducers on Breast Cancer dataset with $minsup$=6.}
%\label{scalability}
%\end{figure}

