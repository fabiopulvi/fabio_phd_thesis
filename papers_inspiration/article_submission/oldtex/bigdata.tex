In this review we analyse and evaluate the most effective approaches developed in Hadoop and Spark. Both of them supports MapReduce paradigm (Hadoop is its Java open source implementation), a distributed programming model introduced by Google to support its data intensive processing. A MapReduce application is divided into two major phases which are known as map and reduce. The map function is fed with a shard of the input dataset as series of key/value pairs; after the processing it outputs one or more key/value couples. Between the map and reduce phases, data is exchanged between nodes, sorted and aggregated in the shuffle phase. Finally, the reduce phase is run for each unique key and iterates through the values that are associated with that key. 
The main benefit of the paradigm is the optimized shuffle operation that should be exploited to achieve the best performance. However, not all algorithms can be straightforward adapted to MapReduce paradigm. In~\cite{chu2007map}, the authors designed Statistical Query model as a judging condition for an efficient and balanced implementation.
Hadoop has become very popular in the last decades; it allows programmers not to concern about inter-process communication and low-level details but to focus on the problem solving. The success of Hadoop and MapReduce is due to the paradigm that shifts the computation to the data: thanks to the Hadoop Distributed File System (HDFS), it takes advantages of data locality allowing the nodes to process the data they store.
Hadoop, and MapReduce paradigm in general, are not perfect: for instance, iterative processes do not fit efficiently since, often, each iteration matches a reading phase from the disk. This feature is critical when dealing with huge datasets, and this is considered one of the most improvements of the Spark framework.
Apache Spark is a general purpose in-memory distributed platform, enabling machines to cache data and intermediate results in memory, instead of reloading them from the disk at each iteration, through the introduction of Resilient Distributed Datasets (RDD). A RDD is a read-only, partitioned collection of records obtained from another RDD or from HDFS. There is no need to materialize RDDs at all the stages because all the information about how a RDD is derived is collected (the lineage). For this reason, Spark cannot be considered the best platform to develop algorithms characterised by asynchronous fine-grained processes. However, Spark offers more use applications, supporting both graph and streaming processes, abandoning the strictness of MapReduce batch-oriented paradigm, although maintaining a full compatibility with the latter. Deferring from the two-stage data flow model in MapReduce, Spark can provide very flexible directed acyclic graph (DAG) data flow. Spark elasticity is extended also to the supported languages which are Java, Python and Scala, while Hadoop supports only Java.
For these reasons, in the evaluation that we will make in the next chapters, we will always keep a special attention to the Spark implementation.

\subsection{Hadoop and Spark Machine Learning Libraries}
In the last year the success of these distributed platform was supported by the introduction of open source collections of algorithms. These are very precious for the academic world since they allow researchers to save time for implementation or to use the algorithms as reference baseline for further optimizations. We strongly consider the presence of these libraries as a real advantage of a framework. 
Mahout~\cite{Mahout} for Hadoop has represented one of the most popular collection of Machine Learning algorithms, containing implementations in the areas such as clustering, classification, recommendation systems, etc. All the current implementations are based on Hadoop MapReduce but it is encouraging the introduction of other not so strict distributed systems. MADlib~\cite{madlib}, instead, provides a SQL toolkit of algorithms that run over Hadoop.
Finally, MLLib~\cite{MLLib} is the Machine Learning library developed on Spark, and it is rapidly growing up. MLLib allows researchers to exploit Spark special features to implement all those applications that can benefit of them, e.g. fast iterative procedures.
