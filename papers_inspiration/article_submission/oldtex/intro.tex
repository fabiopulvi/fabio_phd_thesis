In the last years, the increasing capabilities of recent applications to produce and store huge amounts of information has changed dramatically the importance of the intelligent analysis of big data. The interest towards data mining, an important set of techniques useful to extract effective and usable knowledge from data, has risen. 
This trend is noticeable in both academic and industrial domain. For researchers, big data analytics scenario is very challenging. Often, indeed, the application of traditional data mining techniques to such type of data is not straightforward.
It is not only an issue of computational power or memory, some of the most popular techniques had to be redesigned from scratches to fit the new environment. 
On the other side, Industries are interested to the strategic benefits that big data could deliver, even directly. In ~\cite{junque2013predictive}, the authors present a study to illustrate that larger data indeed can be more valuable assets for predictive analytics. The deduction is that institutions with larger collections of data and, of course, the skill to take advantage of them, can obtain a competitive advantage over institutions without.
Data mining is the main tool on which big data analytics relies on and it includes many types of techniques. Clustering analysis techniques are a very popular tool useful to discover hidden structures in unlabelled data, frequent itemsets mining and association rules analysis evidence interesting correlations and dependencies, and finally, supervised learning techniques are useful to extract a function or a model that best approximates the distribution of the input dataset, in order to map or label new examples. 
Frequent Itemsets Mining has been, already from the start, an essential tool among data mining techniques. It is useful for discovering frequently co-occurring items according to user given frequency threshold. Existing mining algorithm revealed to be very efficient on typical datasets but very resource intensive when the size of the input dataset grows up. In general, applying data mining techniques to big data collections has often entailed to cope with computational costs that represent a critical bottleneck. Furthermore, the shift towards horizontal scaling in hardware has highlighted the need of distribution/parallelization of this family of techniques. For this reason, we are witnessing the explosion of distributed and parallel approaches, often accompanied with Cloud-based services (e.g. Platform-as-a-Service tools).
MapReduce~\cite{ArticoloMapReduceGoogle} can be considered the most spread approach in the last decade. Designed to cope with very large datasets, MapReduceâ€™s main idea is to share the processing of the data into independent tasks. Hadoop~\cite{HDFS} is the most widely diffused MapReduce implementation. In the last couple of years, instead, Apache Spark~\cite{Zaharia_spark} is the hottest platform for large scale data analytics, outperforming Hadoop performance thanks to its distributed memory abstraction.
Hadoop and Spark are not the only frameworks supporting the parallelization of Data mining algorithms. GraphLab~\cite{graphlab}, Google Pregel~\cite{pregel} and its open-source counterpart Giraph~\cite{giraph} are fault-tolerant, graph-based framework while SimSQL~\cite{simsql}, for instance, exploits an SQL-based approach. 
Distributed systems are popular also because they became very easy to use: Message Passing Interface (MPI)~\cite{mpi}, one of the most adopted framework in academic environment, works efficiently only on very low level programming such as C.
In this work, we analyse the most effective and concrete Hadoop and Spark based frequent itemset mining implementations. In Section~\ref{bigdata} we introduce the framework on which the algorithms are developed while Section~\ref{Preliminaries} deepens the frequent itemset mining problem. Section~\ref{criteria} introduces the analysis dimension with which we want to structure the comparison while in Section~\ref{algorithms} the evaluated algorithms will be disclosed and briefly explained. In Section~\ref{experimental} we benchmark the algorithms with a set of experiments aimed to evaluate the approaches in many different use cases. Finally, Section~\ref{conclusion} concludes the paper with the final evaluation of the approaches.